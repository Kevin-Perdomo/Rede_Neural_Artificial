{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e1de9ff",
   "metadata": {},
   "source": [
    "# üöÄ **05 - Sistema: Interface Unificada**\n",
    "\n",
    "## üéØ **Objetivo:**\n",
    "Sistema completo que integra todos os componentes: Pinecone + RAG + Chains + Router em uma interface √∫nica e intuitiva.\n",
    "\n",
    "## üìã **O que faremos:**\n",
    "1. üîó **Integra√ß√£o Total**: Todos os notebooks anteriores unidos\n",
    "2. üí¨ **Interface de Chat**: Conversa√ß√£o natural com usu√°rio\n",
    "3. üìä **M√©tricas em Tempo Real**: Performance e estat√≠sticas\n",
    "4. üé® **Demo Interativa**: Demonstra√ß√£o completa do sistema\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20c8d52",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ **Setup Completo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d473db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports completos\n",
    "from pinecone import Pinecone\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Carregar vari√°veis de ambiente\n",
    "load_dotenv()\n",
    "\n",
    "print(\"üì¶ **SETUP SISTEMA COMPLETO**\")\n",
    "print(\"‚úÖ Bibliotecas importadas\")\n",
    "print(\"‚úÖ Vari√°veis de ambiente carregadas\")\n",
    "\n",
    "# Verificar credenciais\n",
    "required_keys = ['PINECONE_API_KEY', 'GROQ_API_KEY']\n",
    "missing_keys = [key for key in required_keys if not os.getenv(key)]\n",
    "\n",
    "if missing_keys:\n",
    "    print(f\"‚ö†Ô∏è Configure as chaves: {', '.join(missing_keys)}\")\n",
    "else:\n",
    "    print(\"‚úÖ Todas as credenciais encontradas\")\n",
    "\n",
    "print(\"\\nüöÄ **INICIALIZANDO COMPONENTES...**\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e1d173",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ **Componente 1: Pinecone (Base Vetorial)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e11f9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar Pinecone\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "INDEX_NAME = \"turismo-inteligente\"\n",
    "\n",
    "try:\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    index = pc.Index(INDEX_NAME)\n",
    "    \n",
    "    # Verificar conex√£o\n",
    "    stats = index.describe_index_stats()\n",
    "    print(f\"üìå **PINECONE CONECTADO:**\")\n",
    "    print(f\"   üóÑÔ∏è √çndice: {INDEX_NAME}\")\n",
    "    print(f\"   üìä Vetores: {stats['total_vector_count']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro Pinecone: {e}\")\n",
    "    index = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a25de84",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ **Componente 2: Modelos (LLM + Embeddings)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7610d96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar modelos\n",
    "GROQ_API_KEY = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "try:\n",
    "    # LLM para gera√ß√£o\n",
    "    llm_geracao = ChatGroq(\n",
    "        groq_api_key=GROQ_API_KEY,\n",
    "        model_name=\"llama-3.1-8b-instant\",\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    # LLM para classifica√ß√£o (temperatura zero)\n",
    "    llm_router = ChatGroq(\n",
    "        groq_api_key=GROQ_API_KEY,\n",
    "        model_name=\"llama-3.1-8b-instant\",\n",
    "        temperature=0.0\n",
    "    )\n",
    "    \n",
    "    # Embeddings\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        model_kwargs={'device': 'cpu'}\n",
    "    )\n",
    "    \n",
    "    print(f\"üß† **MODELOS CARREGADOS:**\")\n",
    "    print(f\"   ‚ö° LLM Gera√ß√£o: llama-3.1-8b-instant (temp=0.1)\")\n",
    "    print(f\"   üéØ LLM Router: llama-3.1-8b-instant (temp=0.0)\")\n",
    "    print(f\"   üî¢ Embeddings: all-MiniLM-L6-v2 (384d)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro nos modelos: {e}\")\n",
    "    llm_geracao = llm_router = embeddings = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c985d1",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ **Componente 3: Sistema RAG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78965618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buscar_contexto(query: str, top_k: int = 3, filtros: Dict = None) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Busca contexto relevante no Pinecone.\n",
    "    \"\"\"\n",
    "    if not index or not embeddings:\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Gerar embedding\n",
    "        query_embedding = embeddings.embed_query(query)\n",
    "        \n",
    "        # Buscar no Pinecone\n",
    "        results = index.query(\n",
    "            vector=query_embedding,\n",
    "            top_k=top_k,\n",
    "            include_metadata=True,\n",
    "            filter=filtros\n",
    "        )\n",
    "        \n",
    "        # Processar resultados\n",
    "        documentos = []\n",
    "        for match in results['matches']:\n",
    "            doc = {\n",
    "                'texto': match['metadata']['text'],\n",
    "                'cidade': match['metadata']['cidade'],\n",
    "                'tipo': match['metadata']['tipo'],\n",
    "                'score': match['score']\n",
    "            }\n",
    "            documentos.append(doc)\n",
    "        \n",
    "        return documentos\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erro na busca RAG: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"üîç **SISTEMA RAG CONFIGURADO**\")\n",
    "print(\"   ‚úÖ Fun√ß√£o de busca por similaridade\")\n",
    "print(\"   ‚úÖ Processamento de contexto\")\n",
    "print(\"   ‚úÖ Integra√ß√£o Pinecone + Embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0594ef",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ **Componente 4: Chains Especializadas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90db102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Templates das chains especializadas\n",
    "templates = {\n",
    "    'roteiro': \"\"\"\n",
    "üó∫Ô∏è ESPECIALISTA EM ROTEIROS TUR√çSTICOS\n",
    "\n",
    "Contexto relevante:\n",
    "{contexto}\n",
    "\n",
    "Pergunta: {query}\n",
    "\n",
    "Como especialista em roteiros, forne√ßa informa√ß√µes detalhadas sobre pontos tur√≠sticos, \n",
    "atra√ß√µes e roteiros otimizados. Seja espec√≠fico e pr√°tico.\n",
    "\n",
    "Resposta:\"\"\",\n",
    "\n",
    "    'logistica': \"\"\"\n",
    "üöó ESPECIALISTA EM LOG√çSTICA DE VIAGEM\n",
    "\n",
    "Contexto relevante:\n",
    "{contexto}\n",
    "\n",
    "Pergunta: {query}\n",
    "\n",
    "Como especialista em log√≠stica, forne√ßa informa√ß√µes pr√°ticas sobre transporte, \n",
    "hospedagem e locomo√ß√£o. Inclua detalhes operacionais √∫teis.\n",
    "\n",
    "Resposta:\"\"\",\n",
    "\n",
    "    'info-local': \"\"\"\n",
    "üìç ESPECIALISTA EM INFORMA√á√ïES LOCAIS\n",
    "\n",
    "Contexto relevante:\n",
    "{contexto}\n",
    "\n",
    "Pergunta: {query}\n",
    "\n",
    "Como conhecedor local, compartilhe insights sobre cultura, costumes, gastronomia \n",
    "e dicas que s√≥ um local saberia. Seja caloroso e informativo.\n",
    "\n",
    "Resposta:\"\"\",\n",
    "\n",
    "    'traducao': \"\"\"\n",
    "üåê ESPECIALISTA EM TRADU√á√ÉO E COMUNICA√á√ÉO\n",
    "\n",
    "Contexto relevante:\n",
    "{contexto}\n",
    "\n",
    "Pergunta: {query}\n",
    "\n",
    "Como especialista em tradu√ß√£o, forne√ßa tradu√ß√µes precisas, frases √∫teis \n",
    "e dicas de comunica√ß√£o. Inclua pron√∫ncia quando poss√≠vel.\n",
    "\n",
    "Resposta:\"\"\"\n",
    "}\n",
    "\n",
    "# Criar chains\n",
    "chains = {}\n",
    "for nome, template in templates.items():\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"contexto\", \"query\"],\n",
    "        template=template\n",
    "    )\n",
    "    chain = LLMChain(llm=llm_geracao, prompt=prompt, verbose=False)\n",
    "    chains[nome] = chain\n",
    "\n",
    "print(f\"‚õìÔ∏è **CHAINS ESPECIALIZADAS CRIADAS:**\")\n",
    "for nome in chains.keys():\n",
    "    print(f\"   ‚úÖ Chain {nome}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d377d9",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ **Componente 5: Sistema de Router**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dc2bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template do router\n",
    "template_router = \"\"\"\n",
    "Voc√™ √© um classificador de inten√ß√µes para consultas tur√≠sticas.\n",
    "\n",
    "ESPECIALIDADES:\n",
    "- roteiro: pontos tur√≠sticos, atra√ß√µes, onde ir, o que visitar\n",
    "- logistica: transporte, hospedagem, como chegar, aeroporto, hotel\n",
    "- info-local: cultura, costumes, comida, seguran√ßa, dicas locais\n",
    "- traducao: traduzir, idiomas, como dizer, frases √∫teis\n",
    "\n",
    "CONSULTA: \"{query}\"\n",
    "\n",
    "Responda APENAS com o nome da especialidade (roteiro, logistica, info-local, ou traducao):\n",
    "\"\"\"\n",
    "\n",
    "# Criar chain do router\n",
    "prompt_router = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=template_router\n",
    ")\n",
    "\n",
    "chain_router = LLMChain(\n",
    "    llm=llm_router,\n",
    "    prompt=prompt_router,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "def classificar_query(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Classifica a query e retorna a especialidade.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        resultado = chain_router.run({\"query\": query})\n",
    "        especialidade = resultado.strip().lower()\n",
    "        \n",
    "        # Validar resultado\n",
    "        if especialidade in chains:\n",
    "            return especialidade\n",
    "        \n",
    "        # Fallback simples\n",
    "        query_lower = query.lower()\n",
    "        if any(word in query_lower for word in ['hotel', 'aeroporto', 'transporte']):\n",
    "            return 'logistica'\n",
    "        elif any(word in query_lower for word in ['traduz', 'dizer', 'falar']):\n",
    "            return 'traducao'\n",
    "        elif any(word in query_lower for word in ['cultura', 'comida', 'costume']):\n",
    "            return 'info-local'\n",
    "        else:\n",
    "            return 'roteiro'\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Erro no router: {e}\")\n",
    "        return 'roteiro'\n",
    "\n",
    "print(\"üéØ **SISTEMA DE ROUTER CONFIGURADO**\")\n",
    "print(\"   ‚úÖ Chain de classifica√ß√£o\")\n",
    "print(\"   ‚úÖ Fallback autom√°tico\")\n",
    "print(\"   ‚úÖ Valida√ß√£o de resultados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4791822",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ **Sistema Completo Integrado**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cca43a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GuiaTurismoInteligente:\n",
    "    \"\"\"\n",
    "    Sistema completo de guia tur√≠stico inteligente.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stats = {\n",
    "            'consultas_total': 0,\n",
    "            'por_especialidade': {'roteiro': 0, 'logistica': 0, 'info-local': 0, 'traducao': 0},\n",
    "            'tempo_medio': 0,\n",
    "            'historico': []\n",
    "        }\n",
    "    \n",
    "    def processar_consulta(self, query: str, mostrar_debug: bool = False) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Processa uma consulta completa do usu√°rio.\n",
    "        \n",
    "        Returns:\n",
    "            Dicion√°rio com resposta e metadados\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # 1. Classificar consulta\n",
    "            especialidade = classificar_query(query)\n",
    "            \n",
    "            if mostrar_debug:\n",
    "                print(f\"üéØ Roteamento: {especialidade}\")\n",
    "            \n",
    "            # 2. Buscar contexto relevante\n",
    "            documentos = buscar_contexto(query, top_k=3)\n",
    "            contexto = \"\\n\".join([\n",
    "                f\"‚Ä¢ {doc['cidade'].upper()}: {doc['texto']}\"\n",
    "                for doc in documentos\n",
    "            ]) if documentos else \"Contexto n√£o encontrado.\"\n",
    "            \n",
    "            if mostrar_debug:\n",
    "                print(f\"üîç Contexto: {len(documentos)} documentos\")\n",
    "            \n",
    "            # 3. Executar chain especializada\n",
    "            if especialidade in chains:\n",
    "                resposta = chains[especialidade].run({\n",
    "                    \"contexto\": contexto,\n",
    "                    \"query\": query\n",
    "                })\n",
    "            else:\n",
    "                resposta = \"‚ùå Especialidade n√£o encontrada.\"\n",
    "            \n",
    "            # 4. Calcular m√©tricas\n",
    "            tempo_processamento = time.time() - start_time\n",
    "            \n",
    "            # 5. Atualizar estat√≠sticas\n",
    "            self.stats['consultas_total'] += 1\n",
    "            self.stats['por_especialidade'][especialidade] += 1\n",
    "            self.stats['tempo_medio'] = (\n",
    "                (self.stats['tempo_medio'] * (self.stats['consultas_total'] - 1) + tempo_processamento) / \n",
    "                self.stats['consultas_total']\n",
    "            )\n",
    "            \n",
    "            # 6. Salvar no hist√≥rico\n",
    "            entry = {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'query': query,\n",
    "                'especialidade': especialidade,\n",
    "                'tempo_ms': tempo_processamento * 1000,\n",
    "                'contextos_encontrados': len(documentos)\n",
    "            }\n",
    "            self.stats['historico'].append(entry)\n",
    "            \n",
    "            return {\n",
    "                'resposta': resposta,\n",
    "                'especialidade': especialidade,\n",
    "                'contextos': len(documentos),\n",
    "                'tempo_ms': tempo_processamento * 1000,\n",
    "                'sucesso': True\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'resposta': f\"‚ùå Erro no processamento: {e}\",\n",
    "                'especialidade': None,\n",
    "                'contextos': 0,\n",
    "                'tempo_ms': (time.time() - start_time) * 1000,\n",
    "                'sucesso': False\n",
    "            }\n",
    "    \n",
    "    def consultar(self, pergunta: str) -> str:\n",
    "        \"\"\"\n",
    "        Interface simples para consulta.\n",
    "        \"\"\"\n",
    "        resultado = self.processar_consulta(pergunta)\n",
    "        return resultado['resposta']\n",
    "    \n",
    "    def mostrar_estatisticas(self):\n",
    "        \"\"\"\n",
    "        Exibe estat√≠sticas do sistema.\n",
    "        \"\"\"\n",
    "        print(\"üìä **ESTAT√çSTICAS DO SISTEMA:**\")\n",
    "        print(f\"   üìà Total de consultas: {self.stats['consultas_total']}\")\n",
    "        print(f\"   ‚ö° Tempo m√©dio: {self.stats['tempo_medio']*1000:.1f}ms\")\n",
    "        print(\"\\n   üéØ **Por especialidade:**\")\n",
    "        for esp, count in self.stats['por_especialidade'].items():\n",
    "            print(f\"     ‚Ä¢ {esp}: {count}\")\n",
    "\n",
    "# Inicializar sistema\n",
    "guia = GuiaTurismoInteligente()\n",
    "\n",
    "print(\"üöÄ **SISTEMA COMPLETO INICIALIZADO!**\")\n",
    "print(\"   ‚úÖ Pinecone + RAG + Chains + Router\")\n",
    "print(\"   ‚úÖ Interface unificada\")\n",
    "print(\"   ‚úÖ M√©tricas em tempo real\")\n",
    "print(\"   ‚úÖ Pronto para uso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cebe7a",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ **Demo Interativa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03246b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstra√ß√£o completa do sistema\n",
    "print(\"üé¨ **DEMONSTRA√á√ÉO DO SISTEMA COMPLETO**\\n\")\n",
    "\n",
    "consultas_demo = [\n",
    "    \"Quais s√£o os principais pontos tur√≠sticos do Rio de Janeiro?\",\n",
    "    \"Como ir do aeroporto Charles de Gaulle para o centro de Paris?\",\n",
    "    \"Que pratos t√≠picos franceses devo experimentar?\",\n",
    "    \"Como dizer 'onde fica o banheiro' em franc√™s?\"\n",
    "]\n",
    "\n",
    "for i, pergunta in enumerate(consultas_demo, 1):\n",
    "    print(f\"üí¨ **CONSULTA {i}:**\")\n",
    "    print(f\"üë§ Usuario: {pergunta}\\n\")\n",
    "    \n",
    "    # Processar com debug\n",
    "    resultado = guia.processar_consulta(pergunta, mostrar_debug=True)\n",
    "    \n",
    "    print(f\"ü§ñ **Resposta ({resultado['especialidade']}) - {resultado['tempo_ms']:.0f}ms:**\")\n",
    "    print(resultado['resposta'])\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Mostrar estat√≠sticas finais\n",
    "guia.mostrar_estatisticas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac77bd5",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ **Interface de Chat Simples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f42749e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_interativo(max_interacoes: int = 3):\n",
    "    \"\"\"\n",
    "    Interface de chat simples para intera√ß√£o com o usu√°rio.\n",
    "    \"\"\"\n",
    "    print(\"üí¨ **CHAT INTERATIVO ATIVADO!**\")\n",
    "    print(\"üéØ Pergunte sobre turismo (Rio ou Paris)\")\n",
    "    print(\"‚úã Digite 'sair' para encerrar\\n\")\n",
    "    \n",
    "    # Perguntas de exemplo (para demonstra√ß√£o)\n",
    "    perguntas_exemplo = [\n",
    "        \"O que visitar no Rio em 2 dias?\",\n",
    "        \"Melhor forma de se locomover em Paris?\",\n",
    "        \"Dicas de seguran√ßa para turistas?\"\n",
    "    ]\n",
    "    \n",
    "    for i in range(max_interacoes):\n",
    "        # Simular entrada do usu√°rio\n",
    "        if i < len(perguntas_exemplo):\n",
    "            pergunta = perguntas_exemplo[i]\n",
    "            print(f\"üë§ Usu√°rio: {pergunta}\")\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "        # Processar pergunta\n",
    "        resultado = guia.processar_consulta(pergunta)\n",
    "        \n",
    "        # Mostrar resposta\n",
    "        print(f\"ü§ñ Guia ({resultado['especialidade']}): {resultado['resposta']}\\n\")\n",
    "        \n",
    "        # Simular pausa\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    print(\"üëã Chat encerrado!\")\n",
    "    guia.mostrar_estatisticas()\n",
    "\n",
    "# Executar chat demo\n",
    "chat_interativo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0a5951",
   "metadata": {},
   "source": [
    "## üîü **Fun√ß√£o Utilit√°ria Final**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d954415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o de interface mais limpa para uso externo\n",
    "def perguntar_guia(pergunta: str) -> str:\n",
    "    \"\"\"\n",
    "    Interface ultra-simplificada para consultas.\n",
    "    \n",
    "    Usage:\n",
    "        resposta = perguntar_guia(\"O que fazer em Paris?\")\n",
    "        print(resposta)\n",
    "    \"\"\"\n",
    "    return guia.consultar(pergunta)\n",
    "\n",
    "# Testes da interface final\n",
    "print(\"üéØ **TESTE DA INTERFACE FINAL:**\\n\")\n",
    "\n",
    "perguntas_teste = [\n",
    "    \"Principais atra√ß√µes do Rio?\",\n",
    "    \"Como falar 'obrigado' em franc√™s?\"\n",
    "]\n",
    "\n",
    "for pergunta in perguntas_teste:\n",
    "    print(f\"‚ùì {pergunta}\")\n",
    "    resposta = perguntar_guia(pergunta)\n",
    "    print(f\"üí¨ {resposta}\\n\")\n",
    "    print(\"-\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f310fa3",
   "metadata": {},
   "source": [
    "## ‚úÖ **Resumo do Notebook 05:**\n",
    "\n",
    "### üöÄ **Sistema Completo Implementado:**\n",
    "- üìå **Pinecone**: Base vetorial com dados tur√≠sticos\n",
    "- üîç **RAG**: Recupera√ß√£o inteligente de contexto\n",
    "- ‚õìÔ∏è **Chains**: 4 especialistas (roteiro, log√≠stica, info-local, tradu√ß√£o)\n",
    "- üéØ **Router**: Classifica√ß√£o autom√°tica de inten√ß√µes\n",
    "- ü§ñ **Interface**: Sistema unificado e intuitivo\n",
    "\n",
    "### üìä **M√©tricas do Sistema:**\n",
    "- ‚ö° **Velocidade**: ~500-2000ms por consulta\n",
    "- üéØ **Precis√£o**: Alta especializa√ß√£o por dom√≠nio\n",
    "- üîÑ **Robustez**: Fallbacks autom√°ticos\n",
    "- üìà **Escalabilidade**: F√°cil adi√ß√£o de novos especialistas\n",
    "\n",
    "### üõ†Ô∏è **Classes e Fun√ß√µes Principais:**\n",
    "1. **GuiaTurismoInteligente**: Sistema principal completo\n",
    "2. **processar_consulta()**: Fluxo completo com m√©tricas\n",
    "3. **consultar()**: Interface simples\n",
    "4. **perguntar_guia()**: Interface ultra-simplificada\n",
    "5. **chat_interativo()**: Demo de conversa√ß√£o\n",
    "\n",
    "### üß™ **Demonstra√ß√µes Realizadas:**\n",
    "- ‚úÖ Sistema completo end-to-end\n",
    "- ‚úÖ Chat interativo funcional\n",
    "- ‚úÖ M√©tricas em tempo real\n",
    "- ‚úÖ Interface limpa para uso externo\n",
    "\n",
    "### üéØ **Fluxo Completo:**\n",
    "```\n",
    "Usu√°rio ‚Üí Router ‚Üí RAG ‚Üí Chain Especializada ‚Üí Resposta\n",
    "```\n",
    "\n",
    "---\n",
    "üéâ **SISTEMA TUR√çSTICO INTELIGENTE COMPLETO!**\n",
    "\n",
    "**Para usar:**\n",
    "```python\n",
    "resposta = perguntar_guia(\"O que visitar no Rio?\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
