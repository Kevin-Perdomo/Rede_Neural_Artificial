{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e1de9ff",
   "metadata": {},
   "source": [
    "# ğŸš€ **05 - Sistema: Interface Unificada**\n",
    "\n",
    "## ğŸ¯ **Objetivo:**\n",
    "Sistema completo que integra todos os componentes: Pinecone + RAG + Chains + Router em uma interface Ãºnica e intuitiva.\n",
    "\n",
    "## ğŸ“‹ **O que faremos:**\n",
    "1. ğŸ”— **IntegraÃ§Ã£o Total**: Todos os notebooks anteriores unidos\n",
    "2. ğŸ’¬ **Interface de Chat**: ConversaÃ§Ã£o natural com usuÃ¡rio\n",
    "3. ğŸ“Š **MÃ©tricas em Tempo Real**: Performance e estatÃ­sticas\n",
    "4. ğŸ¨ **Demo Interativa**: DemonstraÃ§Ã£o completa do sistema\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20c8d52",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ **Setup Completo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d473db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports completos\n",
    "from pinecone import Pinecone\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Carregar variÃ¡veis de ambiente\n",
    "load_dotenv()\n",
    "\n",
    "print(\"ğŸ“¦ **SETUP SISTEMA COMPLETO**\")\n",
    "print(\"âœ… Bibliotecas importadas\")\n",
    "print(\"âœ… VariÃ¡veis de ambiente carregadas\")\n",
    "\n",
    "# Verificar credenciais\n",
    "required_keys = ['PINECONE_API_KEY', 'GROQ_API_KEY']\n",
    "missing_keys = [key for key in required_keys if not os.getenv(key)]\n",
    "\n",
    "if missing_keys:\n",
    "    print(f\"âš ï¸ Configure as chaves: {', '.join(missing_keys)}\")\n",
    "else:\n",
    "    print(\"âœ… Todas as credenciais encontradas\")\n",
    "\n",
    "print(\"\\nğŸš€ **INICIALIZANDO COMPONENTES...**\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e1d173",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ **Componente 1: Pinecone (Base Vetorial)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e11f9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar Pinecone\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "INDEX_NAME = \"turismo-inteligente\"\n",
    "\n",
    "try:\n",
    "    pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "    index = pc.Index(INDEX_NAME)\n",
    "    \n",
    "    # Verificar conexÃ£o\n",
    "    stats = index.describe_index_stats()\n",
    "    print(f\"ğŸ“Œ **PINECONE CONECTADO:**\")\n",
    "    print(f\"   ğŸ—„ï¸ Ãndice: {INDEX_NAME}\")\n",
    "    print(f\"   ğŸ“Š Vetores: {stats['total_vector_count']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erro Pinecone: {e}\")\n",
    "    index = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a25de84",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ **Componente 2: Modelos (LLM + Embeddings)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7610d96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar modelos\n",
    "GROQ_API_KEY = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "try:\n",
    "    # LLM para geraÃ§Ã£o\n",
    "    llm_geracao = ChatGroq(\n",
    "        groq_api_key=GROQ_API_KEY,\n",
    "        model_name=\"llama-3.1-8b-instant\",\n",
    "        temperature=0.1\n",
    "    )\n",
    "    \n",
    "    # LLM para classificaÃ§Ã£o (temperatura zero)\n",
    "    llm_router = ChatGroq(\n",
    "        groq_api_key=GROQ_API_KEY,\n",
    "        model_name=\"llama-3.1-8b-instant\",\n",
    "        temperature=0.0\n",
    "    )\n",
    "    \n",
    "    # Embeddings\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        model_kwargs={'device': 'cpu'}\n",
    "    )\n",
    "    \n",
    "    print(f\"ğŸ§  **MODELOS CARREGADOS:**\")\n",
    "    print(f\"   âš¡ LLM GeraÃ§Ã£o: llama-3.1-8b-instant (temp=0.1)\")\n",
    "    print(f\"   ğŸ¯ LLM Router: llama-3.1-8b-instant (temp=0.0)\")\n",
    "    print(f\"   ğŸ”¢ Embeddings: all-MiniLM-L6-v2 (384d)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erro nos modelos: {e}\")\n",
    "    llm_geracao = llm_router = embeddings = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c985d1",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ **Componente 3: Sistema RAG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78965618",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buscar_contexto(query: str, top_k: int = 3, filtros: Dict = None) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Busca contexto relevante no Pinecone.\n",
    "    \"\"\"\n",
    "    if not index or not embeddings:\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        # Gerar embedding\n",
    "        query_embedding = embeddings.embed_query(query)\n",
    "        \n",
    "        # Buscar no Pinecone\n",
    "        results = index.query(\n",
    "            vector=query_embedding,\n",
    "            top_k=top_k,\n",
    "            include_metadata=True,\n",
    "            filter=filtros\n",
    "        )\n",
    "        \n",
    "        # Processar resultados\n",
    "        documentos = []\n",
    "        for match in results['matches']:\n",
    "            doc = {\n",
    "                'texto': match['metadata']['text'],\n",
    "                'cidade': match['metadata']['cidade'],\n",
    "                'tipo': match['metadata']['tipo'],\n",
    "                'score': match['score']\n",
    "            }\n",
    "            documentos.append(doc)\n",
    "        \n",
    "        return documentos\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Erro na busca RAG: {e}\")\n",
    "        return []\n",
    "\n",
    "print(\"ğŸ” **SISTEMA RAG CONFIGURADO**\")\n",
    "print(\"   âœ… FunÃ§Ã£o de busca por similaridade\")\n",
    "print(\"   âœ… Processamento de contexto\")\n",
    "print(\"   âœ… IntegraÃ§Ã£o Pinecone + Embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0594ef",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ **Componente 4: Chains Especializadas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90db102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Templates das chains especializadas\n",
    "templates = {\n",
    "    'roteiro': \"\"\"\n",
    "ğŸ—ºï¸ ESPECIALISTA EM ROTEIROS TURÃSTICOS\n",
    "\n",
    "Contexto relevante:\n",
    "{contexto}\n",
    "\n",
    "Pergunta: {query}\n",
    "\n",
    "Como especialista em roteiros, forneÃ§a informaÃ§Ãµes detalhadas sobre pontos turÃ­sticos, \n",
    "atraÃ§Ãµes e roteiros otimizados. Seja especÃ­fico e prÃ¡tico.\n",
    "\n",
    "Resposta:\"\"\",\n",
    "\n",
    "    'logistica': \"\"\"\n",
    "ğŸš— ESPECIALISTA EM LOGÃSTICA DE VIAGEM\n",
    "\n",
    "Contexto relevante:\n",
    "{contexto}\n",
    "\n",
    "Pergunta: {query}\n",
    "\n",
    "Como especialista em logÃ­stica, forneÃ§a informaÃ§Ãµes prÃ¡ticas sobre transporte, \n",
    "hospedagem e locomoÃ§Ã£o. Inclua detalhes operacionais Ãºteis.\n",
    "\n",
    "Resposta:\"\"\",\n",
    "\n",
    "    'info-local': \"\"\"\n",
    "ğŸ“ ESPECIALISTA EM INFORMAÃ‡Ã•ES LOCAIS\n",
    "\n",
    "Contexto relevante:\n",
    "{contexto}\n",
    "\n",
    "Pergunta: {query}\n",
    "\n",
    "Como conhecedor local, compartilhe insights sobre cultura, costumes, gastronomia \n",
    "e dicas que sÃ³ um local saberia. Seja caloroso e informativo.\n",
    "\n",
    "Resposta:\"\"\",\n",
    "\n",
    "    'traducao': \"\"\"\n",
    "ğŸŒ ESPECIALISTA EM TRADUÃ‡ÃƒO E COMUNICAÃ‡ÃƒO\n",
    "\n",
    "Contexto relevante:\n",
    "{contexto}\n",
    "\n",
    "Pergunta: {query}\n",
    "\n",
    "Como especialista em traduÃ§Ã£o, forneÃ§a traduÃ§Ãµes precisas, frases Ãºteis \n",
    "e dicas de comunicaÃ§Ã£o. Inclua pronÃºncia quando possÃ­vel.\n",
    "\n",
    "Resposta:\"\"\"\n",
    "}\n",
    "\n",
    "# Criar chains\n",
    "chains = {}\n",
    "for nome, template in templates.items():\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"contexto\", \"query\"],\n",
    "        template=template\n",
    "    )\n",
    "    chain = LLMChain(llm=llm_geracao, prompt=prompt, verbose=False)\n",
    "    chains[nome] = chain\n",
    "\n",
    "print(f\"â›“ï¸ **CHAINS ESPECIALIZADAS CRIADAS:**\")\n",
    "for nome in chains.keys():\n",
    "    print(f\"   âœ… Chain {nome}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d377d9",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ **Componente 5: Sistema de Router**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dc2bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template do router\n",
    "template_router = \"\"\"\n",
    "VocÃª Ã© um classificador de intenÃ§Ãµes para consultas turÃ­sticas.\n",
    "\n",
    "ESPECIALIDADES:\n",
    "- roteiro: pontos turÃ­sticos, atraÃ§Ãµes, onde ir, o que visitar\n",
    "- logistica: transporte, hospedagem, como chegar, aeroporto, hotel\n",
    "- info-local: cultura, costumes, comida, seguranÃ§a, dicas locais\n",
    "- traducao: traduzir, idiomas, como dizer, frases Ãºteis\n",
    "\n",
    "CONSULTA: \"{query}\"\n",
    "\n",
    "Responda APENAS com o nome da especialidade (roteiro, logistica, info-local, ou traducao):\n",
    "\"\"\"\n",
    "\n",
    "# Criar chain do router\n",
    "prompt_router = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=template_router\n",
    ")\n",
    "\n",
    "chain_router = LLMChain(\n",
    "    llm=llm_router,\n",
    "    prompt=prompt_router,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "def classificar_query(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Classifica a query e retorna a especialidade.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        resultado = chain_router.run({\"query\": query})\n",
    "        especialidade = resultado.strip().lower()\n",
    "        \n",
    "        # Validar resultado\n",
    "        if especialidade in chains:\n",
    "            return especialidade\n",
    "        \n",
    "        # Fallback simples\n",
    "        query_lower = query.lower()\n",
    "        if any(word in query_lower for word in ['hotel', 'aeroporto', 'transporte']):\n",
    "            return 'logistica'\n",
    "        elif any(word in query_lower for word in ['traduz', 'dizer', 'falar']):\n",
    "            return 'traducao'\n",
    "        elif any(word in query_lower for word in ['cultura', 'comida', 'costume']):\n",
    "            return 'info-local'\n",
    "        else:\n",
    "            return 'roteiro'\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Erro no router: {e}\")\n",
    "        return 'roteiro'\n",
    "\n",
    "print(\"ğŸ¯ **SISTEMA DE ROUTER CONFIGURADO**\")\n",
    "print(\"   âœ… Chain de classificaÃ§Ã£o\")\n",
    "print(\"   âœ… Fallback automÃ¡tico\")\n",
    "print(\"   âœ… ValidaÃ§Ã£o de resultados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4791822",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ **Sistema Completo Integrado**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cca43a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GuiaTurismoInteligente:\n",
    "    \"\"\"\n",
    "    Sistema completo de guia turÃ­stico inteligente.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.stats = {\n",
    "            'consultas_total': 0,\n",
    "            'por_especialidade': {'roteiro': 0, 'logistica': 0, 'info-local': 0, 'traducao': 0},\n",
    "            'tempo_medio': 0,\n",
    "            'historico': []\n",
    "        }\n",
    "    \n",
    "    def processar_consulta(self, query: str, mostrar_debug: bool = False) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Processa uma consulta completa do usuÃ¡rio.\n",
    "        \n",
    "        Returns:\n",
    "            DicionÃ¡rio com resposta e metadados\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # 1. Classificar consulta\n",
    "            especialidade = classificar_query(query)\n",
    "            \n",
    "            if mostrar_debug:\n",
    "                print(f\"ğŸ¯ Roteamento: {especialidade}\")\n",
    "            \n",
    "            # 2. Buscar contexto relevante\n",
    "            documentos = buscar_contexto(query, top_k=3)\n",
    "            contexto = \"\\n\".join([\n",
    "                f\"â€¢ {doc['cidade'].upper()}: {doc['texto']}\"\n",
    "                for doc in documentos\n",
    "            ]) if documentos else \"Contexto nÃ£o encontrado.\"\n",
    "            \n",
    "            if mostrar_debug:\n",
    "                print(f\"ğŸ” Contexto: {len(documentos)} documentos\")\n",
    "            \n",
    "            # 3. Executar chain especializada\n",
    "            if especialidade in chains:\n",
    "                resposta = chains[especialidade].run({\n",
    "                    \"contexto\": contexto,\n",
    "                    \"query\": query\n",
    "                })\n",
    "            else:\n",
    "                resposta = \"âŒ Especialidade nÃ£o encontrada.\"\n",
    "            \n",
    "            # 4. Calcular mÃ©tricas\n",
    "            tempo_processamento = time.time() - start_time\n",
    "            \n",
    "            # 5. Atualizar estatÃ­sticas\n",
    "            self.stats['consultas_total'] += 1\n",
    "            self.stats['por_especialidade'][especialidade] += 1\n",
    "            self.stats['tempo_medio'] = (\n",
    "                (self.stats['tempo_medio'] * (self.stats['consultas_total'] - 1) + tempo_processamento) / \n",
    "                self.stats['consultas_total']\n",
    "            )\n",
    "            \n",
    "            # 6. Salvar no histÃ³rico\n",
    "            entry = {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'query': query,\n",
    "                'especialidade': especialidade,\n",
    "                'tempo_ms': tempo_processamento * 1000,\n",
    "                'contextos_encontrados': len(documentos)\n",
    "            }\n",
    "            self.stats['historico'].append(entry)\n",
    "            \n",
    "            return {\n",
    "                'resposta': resposta,\n",
    "                'especialidade': especialidade,\n",
    "                'contextos': len(documentos),\n",
    "                'tempo_ms': tempo_processamento * 1000,\n",
    "                'sucesso': True\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'resposta': f\"âŒ Erro no processamento: {e}\",\n",
    "                'especialidade': None,\n",
    "                'contextos': 0,\n",
    "                'tempo_ms': (time.time() - start_time) * 1000,\n",
    "                'sucesso': False\n",
    "            }\n",
    "    \n",
    "    def consultar(self, pergunta: str) -> str:\n",
    "        \"\"\"\n",
    "        Interface simples para consulta.\n",
    "        \"\"\"\n",
    "        resultado = self.processar_consulta(pergunta)\n",
    "        return resultado['resposta']\n",
    "    \n",
    "    def mostrar_estatisticas(self):\n",
    "        \"\"\"\n",
    "        Exibe estatÃ­sticas do sistema.\n",
    "        \"\"\"\n",
    "        print(\"ğŸ“Š **ESTATÃSTICAS DO SISTEMA:**\")\n",
    "        print(f\"   ğŸ“ˆ Total de consultas: {self.stats['consultas_total']}\")\n",
    "        print(f\"   âš¡ Tempo mÃ©dio: {self.stats['tempo_medio']*1000:.1f}ms\")\n",
    "        print(\"\\n   ğŸ¯ **Por especialidade:**\")\n",
    "        for esp, count in self.stats['por_especialidade'].items():\n",
    "            print(f\"     â€¢ {esp}: {count}\")\n",
    "\n",
    "# Inicializar sistema\n",
    "guia = GuiaTurismoInteligente()\n",
    "\n",
    "print(\"ğŸš€ **SISTEMA COMPLETO INICIALIZADO!**\")\n",
    "print(\"   âœ… Pinecone + RAG + Chains + Router\")\n",
    "print(\"   âœ… Interface unificada\")\n",
    "print(\"   âœ… MÃ©tricas em tempo real\")\n",
    "print(\"   âœ… Pronto para uso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cebe7a",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ **Demo Interativa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03246b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DemonstraÃ§Ã£o completa do sistema\n",
    "print(\"ğŸ¬ **DEMONSTRAÃ‡ÃƒO DO SISTEMA COMPLETO**\\n\")\n",
    "\n",
    "consultas_demo = [\n",
    "    \"Quais sÃ£o os principais pontos turÃ­sticos do Rio de Janeiro?\",\n",
    "    \"Como ir do aeroporto Charles de Gaulle para o centro de Paris?\",\n",
    "    \"Que pratos tÃ­picos franceses devo experimentar?\",\n",
    "    \"Como dizer 'onde fica o banheiro' em francÃªs?\"\n",
    "]\n",
    "\n",
    "for i, pergunta in enumerate(consultas_demo, 1):\n",
    "    print(f\"ğŸ’¬ **CONSULTA {i}:**\")\n",
    "    print(f\"ğŸ‘¤ Usuario: {pergunta}\\n\")\n",
    "    \n",
    "    # Processar com debug\n",
    "    resultado = guia.processar_consulta(pergunta, mostrar_debug=True)\n",
    "    \n",
    "    print(f\"ğŸ¤– **Resposta ({resultado['especialidade']}) - {resultado['tempo_ms']:.0f}ms:**\")\n",
    "    print(resultado['resposta'])\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "# Mostrar estatÃ­sticas finais\n",
    "guia.mostrar_estatisticas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac77bd5",
   "metadata": {},
   "source": [
    "## 9ï¸âƒ£ **Interface de Chat Simples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f42749e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_interativo(max_interacoes: int = 3):\n",
    "    \"\"\"\n",
    "    Interface de chat simples para interaÃ§Ã£o com o usuÃ¡rio.\n",
    "    \"\"\"\n",
    "    print(\"ğŸ’¬ **CHAT INTERATIVO ATIVADO!**\")\n",
    "    print(\"ğŸ¯ Pergunte sobre turismo (Rio ou Paris)\")\n",
    "    print(\"âœ‹ Digite 'sair' para encerrar\\n\")\n",
    "    \n",
    "    # Perguntas de exemplo (para demonstraÃ§Ã£o)\n",
    "    perguntas_exemplo = [\n",
    "        \"O que visitar no Rio em 2 dias?\",\n",
    "        \"Melhor forma de se locomover em Paris?\",\n",
    "        \"Dicas de seguranÃ§a para turistas?\"\n",
    "    ]\n",
    "    \n",
    "    for i in range(max_interacoes):\n",
    "        # Simular entrada do usuÃ¡rio\n",
    "        if i < len(perguntas_exemplo):\n",
    "            pergunta = perguntas_exemplo[i]\n",
    "            print(f\"ğŸ‘¤ UsuÃ¡rio: {pergunta}\")\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "        # Processar pergunta\n",
    "        resultado = guia.processar_consulta(pergunta)\n",
    "        \n",
    "        # Mostrar resposta\n",
    "        print(f\"ğŸ¤– Guia ({resultado['especialidade']}): {resultado['resposta']}\\n\")\n",
    "        \n",
    "        # Simular pausa\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    print(\"ğŸ‘‹ Chat encerrado!\")\n",
    "    guia.mostrar_estatisticas()\n",
    "\n",
    "# Executar chat demo\n",
    "chat_interativo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0a5951",
   "metadata": {},
   "source": [
    "## ğŸ”Ÿ **FunÃ§Ã£o UtilitÃ¡ria Final**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d954415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FunÃ§Ã£o de interface mais limpa para uso externo\n",
    "def perguntar_guia(pergunta: str) -> str:\n",
    "    \"\"\"\n",
    "    Interface ultra-simplificada para consultas.\n",
    "    \n",
    "    Usage:\n",
    "        resposta = perguntar_guia(\"O que fazer em Paris?\")\n",
    "        print(resposta)\n",
    "    \"\"\"\n",
    "    return guia.consultar(pergunta)\n",
    "\n",
    "# Testes da interface final\n",
    "print(\"ğŸ¯ **TESTE DA INTERFACE FINAL:**\\n\")\n",
    "\n",
    "perguntas_teste = [\n",
    "    \"Principais atraÃ§Ãµes do Rio?\",\n",
    "    \"Como falar 'obrigado' em francÃªs?\"\n",
    "]\n",
    "\n",
    "for pergunta in perguntas_teste:\n",
    "    print(f\"â“ {pergunta}\")\n",
    "    resposta = perguntar_guia(pergunta)\n",
    "    print(f\"ğŸ’¬ {resposta}\\n\")\n",
    "    print(\"-\" * 60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f310fa3",
   "metadata": {},
   "source": [
    "## âœ… **Resumo do Notebook 05:**\n",
    "\n",
    "### ğŸš€ **Sistema Completo Implementado:**\n",
    "- ğŸ“Œ **Pinecone**: Base vetorial com dados turÃ­sticos\n",
    "- ğŸ” **RAG**: RecuperaÃ§Ã£o inteligente de contexto\n",
    "- â›“ï¸ **Chains**: 4 especialistas (roteiro, logÃ­stica, info-local, traduÃ§Ã£o)\n",
    "- ğŸ¯ **Router**: ClassificaÃ§Ã£o automÃ¡tica de intenÃ§Ãµes\n",
    "- ğŸ¤– **Interface**: Sistema unificado e intuitivo\n",
    "\n",
    "### ğŸ“Š **MÃ©tricas do Sistema:**\n",
    "- âš¡ **Velocidade**: ~500-2000ms por consulta\n",
    "- ğŸ¯ **PrecisÃ£o**: Alta especializaÃ§Ã£o por domÃ­nio\n",
    "- ğŸ”„ **Robustez**: Fallbacks automÃ¡ticos\n",
    "- ğŸ“ˆ **Escalabilidade**: FÃ¡cil adiÃ§Ã£o de novos especialistas\n",
    "\n",
    "### ğŸ› ï¸ **Classes e FunÃ§Ãµes Principais:**\n",
    "1. **GuiaTurismoInteligente**: Sistema principal completo\n",
    "2. **processar_consulta()**: Fluxo completo com mÃ©tricas\n",
    "3. **consultar()**: Interface simples\n",
    "4. **perguntar_guia()**: Interface ultra-simplificada\n",
    "5. **chat_interativo()**: Demo de conversaÃ§Ã£o\n",
    "\n",
    "### ğŸ§ª **DemonstraÃ§Ãµes Realizadas:**\n",
    "- âœ… Sistema completo end-to-end\n",
    "- âœ… Chat interativo funcional\n",
    "- âœ… MÃ©tricas em tempo real\n",
    "- âœ… Interface limpa para uso externo\n",
    "\n",
    "### ğŸ¯ **Fluxo Completo:**\n",
    "```\n",
    "UsuÃ¡rio â†’ Router â†’ RAG â†’ Chain Especializada â†’ Resposta\n",
    "```\n",
    "\n",
    "---\n",
    "ğŸ‰ **SISTEMA TURÃSTICO INTELIGENTE COMPLETO!**\n",
    "\n",
    "**Para usar:**\n",
    "```python\n",
    "resposta = perguntar_guia(\"O que visitar no Rio?\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
