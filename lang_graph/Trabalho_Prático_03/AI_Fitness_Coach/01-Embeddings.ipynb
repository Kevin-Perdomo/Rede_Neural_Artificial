{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82fde3f8",
   "metadata": {},
   "source": [
    "# üßÆ **01 - Embeddings: Base Vetorial Fitness**\n",
    "\n",
    "## üéØ **Objetivo:**\n",
    "Configurar sistema de embeddings e processar a base de conhecimento fitness.\n",
    "\n",
    "## üìã **O que faremos:**\n",
    "1. ‚öôÔ∏è Setup dos embeddings com SentenceTransformers\n",
    "2. üìÑ Chunking da base de conhecimento\n",
    "3. üßÆ Gera√ß√£o de embeddings vetoriais\n",
    "4. ‚úÖ Testes de similaridade sem√¢ntica\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5901b621",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ **Setup e Verifica√ß√£o de Depend√™ncias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92bf5543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Verificando depend√™ncias para embeddings...\n",
      "‚úÖ sentence_transformers\n",
      "‚úÖ numpy\n",
      "‚úÖ scikit_learn\n",
      "\n",
      "üéâ Todas as depend√™ncias est√£o instaladas!\n",
      "\n",
      "üìã Status: ‚úÖ PRONTO\n"
     ]
    }
   ],
   "source": [
    "# üì¶ VERIFICA√á√ÉO E IMPORTA√á√ÉO DE BIBLIOTECAS\n",
    "\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import json\n",
    "import hashlib\n",
    "from typing import Dict, List, Any, Tuple\n",
    "\n",
    "def verificar_instalacao(pacote):\n",
    "    \"\"\"Verifica se um pacote est√° instalado\"\"\"\n",
    "    try:\n",
    "        importlib.import_module(pacote)\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "# Pacotes essenciais para embeddings\n",
    "pacotes_embeddings = {\n",
    "    'sentence_transformers': 'sentence_transformers',\n",
    "    'numpy': 'numpy',\n",
    "    'scikit_learn': 'sklearn'\n",
    "}\n",
    "\n",
    "print(\"üîç Verificando depend√™ncias para embeddings...\")\n",
    "todos_ok = True\n",
    "for nome_display, nome_modulo in pacotes_embeddings.items():\n",
    "    status = verificar_instalacao(nome_modulo)\n",
    "    emoji = \"‚úÖ\" if status else \"‚ùå\"\n",
    "    print(f\"{emoji} {nome_display}\")\n",
    "    if not status:\n",
    "        todos_ok = False\n",
    "\n",
    "if todos_ok:\n",
    "    print(\"\\nüéâ Todas as depend√™ncias est√£o instaladas!\")\n",
    "else:\n",
    "    print(\"\\nüí° Execute: pip install sentence-transformers scikit-learn\")\n",
    "\n",
    "print(f\"\\nüìã Status: {'‚úÖ PRONTO' if todos_ok else '‚ö†Ô∏è VERIFICAR INSTALA√á√ÉO'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059f8c2b",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ **Carregamento da Base de Conhecimento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7d3d29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Base carregada: 8822 bytes\n",
      "\n",
      "üìä Total de caracteres: 8515\n",
      "üìù Primeiras 200 chars: # BASE DE CONHECIMENTO - FITNESS & MUSCULA√á√ÉO\n",
      "\n",
      "## EXERC√çCIOS POR GRUPO MUSCULAR\n",
      "\n",
      "### PEITO\n",
      "**Supino reto com barra**\n",
      "- M√∫sculos: Peitoral maior, delt√≥ide anterior, tr√≠ceps\n",
      "- Execu√ß√£o: Deite no banco, ...\n"
     ]
    }
   ],
   "source": [
    "# üìÑ CARREGAMENTO DA BASE FITNESS\n",
    "\n",
    "def carregar_base_conhecimento():\n",
    "    \"\"\"Carrega a base de conhecimento fitness do arquivo\"\"\"\n",
    "    arquivo_base = Path(\"base_conhecimento_fitness.txt\")\n",
    "    \n",
    "    if not arquivo_base.exists():\n",
    "        print(\"‚ö†Ô∏è Arquivo base_conhecimento_fitness.txt n√£o encontrado!\")\n",
    "        return criar_base_demo()\n",
    "    \n",
    "    try:\n",
    "        with open(arquivo_base, 'r', encoding='utf-8') as f:\n",
    "            conteudo = f.read()\n",
    "        print(f\"‚úÖ Base carregada: {arquivo_base.stat().st_size} bytes\")\n",
    "        return conteudo\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao carregar: {e}\")\n",
    "        return criar_base_demo()\n",
    "\n",
    "def criar_base_demo():\n",
    "    \"\"\"Cria base m√≠nima para demonstra√ß√£o\"\"\"\n",
    "    return \"\"\"\n",
    "### EXERC√çCIOS B√ÅSICOS\n",
    "\n",
    "**Supino reto**\n",
    "Exerc√≠cio fundamental para peitoral maior, delt√≥ide anterior e tr√≠ceps.\n",
    "\n",
    "**Agachamento**\n",
    "Rei dos exerc√≠cios para quadr√≠ceps, gl√∫teos e core.\n",
    "\n",
    "### HIPERTROFIA\n",
    "S√©ries: 3-4\n",
    "Repeti√ß√µes: 8-12\n",
    "Descanso: 60-90 segundos\n",
    "\"\"\"\n",
    "\n",
    "# Carregar base\n",
    "conhecimento_bruto = carregar_base_conhecimento()\n",
    "print(f\"\\nüìä Total de caracteres: {len(conhecimento_bruto)}\")\n",
    "print(f\"üìù Primeiras 200 chars: {conhecimento_bruto[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c818ef",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ **Chunking Inteligente**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b810e771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÇÔ∏è Processando chunking...\n",
      "\n",
      "üìä ESTAT√çSTICAS DO CHUNKING:\n",
      "   ‚Ä¢ Total de chunks: 22\n",
      "   ‚Ä¢ Tokens m√©dios por chunk: 55.4\n",
      "   ‚Ä¢ Se√ß√µes identificadas: 22\n",
      "\n",
      "üìÑ EXEMPLOS DE CHUNKS:\n",
      "\n",
      "   Chunk 1 (ID: 0, Hash: 01b4d409)\n",
      "   Se√ß√£o: Introdu√ß√£o\n",
      "   Tokens: 13\n",
      "   Texto: # BASE DE CONHECIMENTO - FITNESS & MUSCULA√á√ÉO\n",
      "\n",
      "## EXERC√çCIOS POR GRUPO MUSCULAR...\n",
      "\n",
      "   Chunk 2 (ID: 1, Hash: 85a251ba)\n",
      "   Se√ß√£o: PEITO\n",
      "   Tokens: 168\n",
      "   Texto: ### PEITO\n",
      "**Supino reto com barra**\n",
      "- M√∫sculos: Peitoral maior, delt√≥ide anterior, tr√≠ceps\n",
      "- Execu√ß√£...\n",
      "\n",
      "   Chunk 3 (ID: 2, Hash: ca945e8b)\n",
      "   Se√ß√£o: COSTAS\n",
      "   Tokens: 152\n",
      "   Texto: ### COSTAS\n",
      "**Puxada frontal**\n",
      "- M√∫sculos: Grande dorsal, romb√≥ides, b√≠ceps\n",
      "- Execu√ß√£o: Puxe a barra ...\n"
     ]
    }
   ],
   "source": [
    "# ‚úÇÔ∏è SISTEMA DE CHUNKING INTELIGENTE\n",
    "\n",
    "def fazer_chunking(texto: str, chunk_size: int = 300) -> List[Dict]:\n",
    "    \"\"\"Divide texto em chunks preservando contexto sem√¢ntico\"\"\"\n",
    "    chunks = []\n",
    "    linhas = texto.split('\\n')\n",
    "    \n",
    "    chunk_atual = \"\"\n",
    "    secao_atual = \"Introdu√ß√£o\"\n",
    "    \n",
    "    for linha in linhas:\n",
    "        linha = linha.strip()\n",
    "        \n",
    "        # Detectar nova se√ß√£o (### T√≠tulo)\n",
    "        if linha.startswith('### '):\n",
    "            # Salvar chunk anterior\n",
    "            if chunk_atual.strip():\n",
    "                chunks.append({\n",
    "                    'id': len(chunks),\n",
    "                    'texto': chunk_atual.strip(),\n",
    "                    'secao': secao_atual,\n",
    "                    'tokens': len(chunk_atual.split()),\n",
    "                    'hash': hashlib.md5(chunk_atual.encode()).hexdigest()[:8]\n",
    "                })\n",
    "            \n",
    "            # Nova se√ß√£o\n",
    "            secao_atual = linha.replace('### ', '').strip()\n",
    "            chunk_atual = linha + \"\\n\"\n",
    "            \n",
    "        else:\n",
    "            chunk_atual += linha + \"\\n\"\n",
    "            \n",
    "            # Dividir se muito grande\n",
    "            if len(chunk_atual.split()) > chunk_size:\n",
    "                chunks.append({\n",
    "                    'id': len(chunks),\n",
    "                    'texto': chunk_atual.strip(),\n",
    "                    'secao': secao_atual,\n",
    "                    'tokens': len(chunk_atual.split()),\n",
    "                    'hash': hashlib.md5(chunk_atual.encode()).hexdigest()[:8]\n",
    "                })\n",
    "                chunk_atual = \"\"\n",
    "    \n",
    "    # √öltimo chunk\n",
    "    if chunk_atual.strip():\n",
    "        chunks.append({\n",
    "            'id': len(chunks),\n",
    "            'texto': chunk_atual.strip(),\n",
    "            'secao': secao_atual,\n",
    "            'tokens': len(chunk_atual.split()),\n",
    "            'hash': hashlib.md5(chunk_atual.encode()).hexdigest()[:8]\n",
    "        })\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Processar chunking\n",
    "print(\"‚úÇÔ∏è Processando chunking...\")\n",
    "chunks = fazer_chunking(conhecimento_bruto)\n",
    "\n",
    "print(f\"\\nüìä ESTAT√çSTICAS DO CHUNKING:\")\n",
    "print(f\"   ‚Ä¢ Total de chunks: {len(chunks)}\")\n",
    "print(f\"   ‚Ä¢ Tokens m√©dios por chunk: {sum(c['tokens'] for c in chunks) / len(chunks):.1f}\")\n",
    "print(f\"   ‚Ä¢ Se√ß√µes identificadas: {len(set(c['secao'] for c in chunks))}\")\n",
    "\n",
    "# Mostrar alguns chunks\n",
    "print(f\"\\nüìÑ EXEMPLOS DE CHUNKS:\")\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"\\n   Chunk {i+1} (ID: {chunk['id']}, Hash: {chunk['hash']})\")\n",
    "    print(f\"   Se√ß√£o: {chunk['secao']}\")\n",
    "    print(f\"   Tokens: {chunk['tokens']}\")\n",
    "    print(f\"   Texto: {chunk['texto'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3a0073",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ **Gera√ß√£o de Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5d8adfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Carregando modelo de embeddings...\n",
      "‚úÖ Modelo carregado com sucesso!\n",
      "üìè Dimens√£o dos embeddings: 384\n"
     ]
    }
   ],
   "source": [
    "# üßÆ SISTEMA DE EMBEDDINGS\n",
    "\n",
    "# Importar SentenceTransformers\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    \n",
    "    print(\"üîÑ Carregando modelo de embeddings...\")\n",
    "    # Modelo multilingual otimizado\n",
    "    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    print(\"‚úÖ Modelo carregado com sucesso!\")\n",
    "    print(f\"üìè Dimens√£o dos embeddings: {embedding_model.get_sentence_embedding_dimension()}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ùå SentenceTransformers n√£o dispon√≠vel\")\n",
    "    embedding_model = None\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Erro ao carregar modelo: {e}\")\n",
    "    embedding_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb4c2265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üßÆ Gerando embeddings para todos os chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75c2f5bd4ea347aeba8537c976d3968f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Embeddings gerados!\n",
      "üìä Shape dos embeddings: (22, 384)\n",
      "üìè Dimens√µes: 384\n",
      "üéØ Norma m√©dia: 1.000\n"
     ]
    }
   ],
   "source": [
    "# üéØ GERA√á√ÉO DE EMBEDDINGS VETORIAIS\n",
    "\n",
    "if embedding_model:\n",
    "    print(\"üßÆ Gerando embeddings para todos os chunks...\")\n",
    "    \n",
    "    # Extrair textos dos chunks\n",
    "    textos = [chunk['texto'] for chunk in chunks]\n",
    "    \n",
    "    # Gerar embeddings em batch (mais eficiente)\n",
    "    embeddings = embedding_model.encode(textos, show_progress_bar=True)\n",
    "    \n",
    "    # Adicionar embeddings aos chunks\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk['embedding'] = embeddings[i]\n",
    "        chunk['embedding_norm'] = np.linalg.norm(embeddings[i])  # Norma para refer√™ncia\n",
    "    \n",
    "    print(f\"\\n‚úÖ Embeddings gerados!\")\n",
    "    print(f\"üìä Shape dos embeddings: {embeddings.shape}\")\n",
    "    print(f\"üìè Dimens√µes: {embeddings.shape[1]}\")\n",
    "    print(f\"üéØ Norma m√©dia: {np.mean([c['embedding_norm'] for c in chunks]):.3f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Pulando gera√ß√£o de embeddings - modelo n√£o dispon√≠vel\")\n",
    "    embeddings = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62adfc3c",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ **Testes de Similaridade**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c769c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fun√ß√µes de busca criadas!\n"
     ]
    }
   ],
   "source": [
    "# üîç FUN√á√ÉO DE BUSCA POR SIMILARIDADE\n",
    "\n",
    "def buscar_similares(query: str, chunks: List[Dict], modelo, top_k: int = 3) -> List[Dict]:\n",
    "    \"\"\"Busca chunks similares usando similaridade coseno\"\"\"\n",
    "    \n",
    "    if not modelo or not chunks or 'embedding' not in chunks[0]:\n",
    "        print(\"‚ùå Embeddings n√£o dispon√≠veis - usando busca por palavras\")\n",
    "        return buscar_palavras_chave(query, chunks, top_k)\n",
    "    \n",
    "    try:\n",
    "        # Gerar embedding da query\n",
    "        query_embedding = modelo.encode([query])[0]\n",
    "        \n",
    "        # Calcular similaridades\n",
    "        similaridades = []\n",
    "        for chunk in chunks:\n",
    "            # Similaridade coseno\n",
    "            sim = np.dot(query_embedding, chunk['embedding']) / (\n",
    "                np.linalg.norm(query_embedding) * np.linalg.norm(chunk['embedding'])\n",
    "            )\n",
    "            similaridades.append((sim, chunk))\n",
    "        \n",
    "        # Ordenar por similaridade\n",
    "        similaridades.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        # Retornar top-k com scores\n",
    "        resultados = []\n",
    "        for sim, chunk in similaridades[:top_k]:\n",
    "            resultado = chunk.copy()\n",
    "            resultado['similaridade'] = float(sim)\n",
    "            resultados.append(resultado)\n",
    "        \n",
    "        return resultados\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro na busca sem√¢ntica: {e}\")\n",
    "        return buscar_palavras_chave(query, chunks, top_k)\n",
    "\n",
    "def buscar_palavras_chave(query: str, chunks: List[Dict], top_k: int = 3) -> List[Dict]:\n",
    "    \"\"\"Busca por palavras-chave como fallback\"\"\"\n",
    "    query_lower = query.lower()\n",
    "    query_words = set(query_lower.split())\n",
    "    \n",
    "    resultados = []\n",
    "    for chunk in chunks:\n",
    "        texto_lower = chunk['texto'].lower()\n",
    "        chunk_words = set(texto_lower.split())\n",
    "        \n",
    "        # Score baseado em palavras comuns\n",
    "        palavras_comuns = len(query_words.intersection(chunk_words))\n",
    "        if palavras_comuns > 0:\n",
    "            score = palavras_comuns / len(query_words)\n",
    "            resultado = chunk.copy()\n",
    "            resultado['similaridade'] = score\n",
    "            resultados.append(resultado)\n",
    "    \n",
    "    # Ordenar por score\n",
    "    resultados.sort(key=lambda x: x['similaridade'], reverse=True)\n",
    "    return resultados[:top_k]\n",
    "\n",
    "print(\"‚úÖ Fun√ß√µes de busca criadas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c7760f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ TESTANDO BUSCA SEM√ÇNTICA\n",
      "==================================================\n",
      "\n",
      "1Ô∏è‚É£ Query: 'exerc√≠cios para ganhar m√∫sculos'\n",
      "------------------------------\n",
      "\n",
      "   üìÑ Resultado 1:\n",
      "   üìä Similaridade: 0.575\n",
      "   üìÇ Se√ß√£o: Introdu√ß√£o\n",
      "   üìù Texto: # BASE DE CONHECIMENTO - FITNESS & MUSCULA√á√ÉO\n",
      "\n",
      "## EXERC√çCIOS POR GRUPO MUSCULAR...\n",
      "\n",
      "   üìÑ Resultado 2:\n",
      "   üìä Similaridade: 0.519\n",
      "   üìÇ Se√ß√£o: PERNAS\n",
      "   üìù Texto: ### PERNAS\n",
      "\n",
      "#### QUADR√çCEPS\n",
      "**Agachamento**\n",
      "- M√∫sculos: Quadr√≠ceps, gl√∫teos, core\n",
      "- Execu√ß√£o: Descida controlada at√© 90¬∞...\n",
      "\n",
      "2Ô∏è‚É£ Query: 'treino de peito'\n",
      "------------------------------\n",
      "\n",
      "   üìÑ Resultado 1:\n",
      "   üìä Similaridade: 0.462\n",
      "   üìÇ Se√ß√£o: FOR√áA\n",
      "   üìù Texto: ### FOR√áA\n",
      "- **Volume**: Moderado (3-5 s√©ries, 1-6 repeti√ß√µes)\n",
      "- **Intensidade**: Alta (85-100% 1RM)\n",
      "- **Frequ√™ncia**: 3-...\n",
      "\n",
      "   üìÑ Resultado 2:\n",
      "   üìä Similaridade: 0.459\n",
      "   üìÇ Se√ß√£o: CONDICIONAMENTO\n",
      "   üìù Texto: ### CONDICIONAMENTO\n",
      "- **Volume**: Alto\n",
      "- **Intensidade**: Variada\n",
      "- **Frequ√™ncia**: 5-6x por semana\n",
      "- **Modalidades**: H...\n",
      "\n",
      "3Ô∏è‚É£ Query: 'quantas s√©ries fazer'\n",
      "------------------------------\n",
      "\n",
      "   üìÑ Resultado 1:\n",
      "   üìä Similaridade: 0.320\n",
      "   üìÇ Se√ß√£o: FOR√áA\n",
      "   üìù Texto: ### FOR√áA\n",
      "- **Volume**: Moderado (3-5 s√©ries, 1-6 repeti√ß√µes)\n",
      "- **Intensidade**: Alta (85-100% 1RM)\n",
      "- **Frequ√™ncia**: 3-...\n",
      "\n",
      "   üìÑ Resultado 2:\n",
      "   üìä Similaridade: 0.252\n",
      "   üìÇ Se√ß√£o: HIPERTROFIA\n",
      "   üìù Texto: ### HIPERTROFIA\n",
      "- **Volume**: Alto (3-4 s√©ries, 8-12 repeti√ß√µes)\n",
      "- **Intensidade**: Moderada a alta (70-85% 1RM)\n",
      "- **Fre...\n",
      "\n",
      "4Ô∏è‚É£ Query: 'descanso entre exerc√≠cios'\n",
      "------------------------------\n",
      "\n",
      "   üìÑ Resultado 1:\n",
      "   üìä Similaridade: 0.473\n",
      "   üìÇ Se√ß√£o: Introdu√ß√£o\n",
      "   üìù Texto: # BASE DE CONHECIMENTO - FITNESS & MUSCULA√á√ÉO\n",
      "\n",
      "## EXERC√çCIOS POR GRUPO MUSCULAR...\n",
      "\n",
      "   üìÑ Resultado 2:\n",
      "   üìä Similaridade: 0.455\n",
      "   üìÇ Se√ß√£o: GASTO CAL√ìRICO APROXIMADO POR TREINO\n",
      "   üìù Texto: ### GASTO CAL√ìRICO APROXIMADO POR TREINO\n",
      "**Muscula√ß√£o (por hora):**\n",
      "- Pessoa 70kg: ~220-400 calorias\n",
      "- Pessoa 80kg: ~252...\n",
      "\n",
      "==================================================\n",
      "‚úÖ Testes de similaridade conclu√≠dos!\n"
     ]
    }
   ],
   "source": [
    "# üß™ TESTES DE BUSCA SEM√ÇNTICA\n",
    "\n",
    "print(\"üß™ TESTANDO BUSCA SEM√ÇNTICA\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Queries de teste\n",
    "queries_teste = [\n",
    "    \"exerc√≠cios para ganhar m√∫sculos\",\n",
    "    \"treino de peito\", \n",
    "    \"quantas s√©ries fazer\",\n",
    "    \"descanso entre exerc√≠cios\"\n",
    "]\n",
    "\n",
    "for i, query in enumerate(queries_teste, 1):\n",
    "    print(f\"\\n{i}Ô∏è‚É£ Query: '{query}'\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    resultados = buscar_similares(query, chunks, embedding_model, top_k=2)\n",
    "    \n",
    "    for j, resultado in enumerate(resultados, 1):\n",
    "        print(f\"\\n   üìÑ Resultado {j}:\")\n",
    "        print(f\"   üìä Similaridade: {resultado['similaridade']:.3f}\")\n",
    "        print(f\"   üìÇ Se√ß√£o: {resultado['secao']}\")\n",
    "        print(f\"   üìù Texto: {resultado['texto'][:120]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"‚úÖ Testes de similaridade conclu√≠dos!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6425bcb1",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ **Persist√™ncia dos Dados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24700cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chunks salvos em fitness_chunks.json\n",
      "üìä Total: 22 chunks\n"
     ]
    }
   ],
   "source": [
    "# üíæ SALVAR DADOS PROCESSADOS\n",
    "\n",
    "def salvar_chunks_processados(chunks: List[Dict], arquivo: str = \"fitness_chunks.json\"):\n",
    "    \"\"\"Salva chunks processados em JSON (sem embeddings)\"\"\"\n",
    "    \n",
    "    # Preparar dados para salvamento (sem embeddings numpy)\n",
    "    chunks_para_salvar = []\n",
    "    for chunk in chunks:\n",
    "        chunk_limpo = {\n",
    "            'id': chunk['id'],\n",
    "            'texto': chunk['texto'],\n",
    "            'secao': chunk['secao'], \n",
    "            'tokens': chunk['tokens'],\n",
    "            'hash': chunk['hash'],\n",
    "            'tem_embedding': 'embedding' in chunk\n",
    "        }\n",
    "        chunks_para_salvar.append(chunk_limpo)\n",
    "    \n",
    "    # Salvar metadata\n",
    "    dados = {\n",
    "        'timestamp': str(pd.Timestamp.now()) if 'pd' in globals() else 'unknown',\n",
    "        'total_chunks': len(chunks),\n",
    "        'modelo_embedding': 'all-MiniLM-L6-v2' if embedding_model else None,\n",
    "        'chunks': chunks_para_salvar\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with open(arquivo, 'w', encoding='utf-8') as f:\n",
    "            json.dump(dados, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"‚úÖ Chunks salvos em {arquivo}\")\n",
    "        print(f\"üìä Total: {len(chunks)} chunks\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao salvar: {e}\")\n",
    "\n",
    "# Salvar se temos chunks\n",
    "if chunks:\n",
    "    salvar_chunks_processados(chunks)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nenhum chunk para salvar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a956c91d",
   "metadata": {},
   "source": [
    "## üìä **Resumo da Etapa 01**\n",
    "\n",
    "### ‚úÖ **Conquistas:**\n",
    "- üìÑ Base de conhecimento carregada e processada\n",
    "- ‚úÇÔ∏è Chunking inteligente implementado\n",
    "- üßÆ Embeddings vetoriais gerados\n",
    "- üîç Sistema de busca por similaridade funcional\n",
    "- üíæ Persist√™ncia de dados configurada\n",
    "\n",
    "---\n",
    "\n",
    "üéØ **Base vetorial fitness criada com sucesso!** üèãÔ∏è‚Äç‚ôÇÔ∏è"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
