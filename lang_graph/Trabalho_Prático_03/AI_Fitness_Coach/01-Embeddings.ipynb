{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82fde3f8",
   "metadata": {},
   "source": [
    "# ğŸ§® **01 - Embeddings: Base Vetorial Fitness**\n",
    "\n",
    "## ğŸ¯ **Objetivo:**\n",
    "Configurar sistema de embeddings e processar a base de conhecimento fitness.\n",
    "\n",
    "## ğŸ“‹ **O que faremos:**\n",
    "1. âš™ï¸ Setup dos embeddings com SentenceTransformers\n",
    "2. ğŸ“„ Chunking da base de conhecimento\n",
    "3. ğŸ§® GeraÃ§Ã£o de embeddings vetoriais\n",
    "4. âœ… Testes de similaridade semÃ¢ntica\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5901b621",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ **Setup e VerificaÃ§Ã£o de DependÃªncias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92bf5543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Verificando dependÃªncias para embeddings...\n",
      "âœ… sentence_transformers\n",
      "âœ… numpy\n",
      "âœ… scikit_learn\n",
      "\n",
      "ğŸ‰ Todas as dependÃªncias estÃ£o instaladas!\n",
      "\n",
      "ğŸ“‹ Status: âœ… PRONTO\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“¦ VERIFICAÃ‡ÃƒO E IMPORTAÃ‡ÃƒO DE BIBLIOTECAS\n",
    "\n",
    "import sys\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import json\n",
    "import hashlib\n",
    "from typing import Dict, List, Any, Tuple\n",
    "\n",
    "def verificar_instalacao(pacote):\n",
    "    \"\"\"Verifica se um pacote estÃ¡ instalado\"\"\"\n",
    "    try:\n",
    "        importlib.import_module(pacote)\n",
    "        return True\n",
    "    except ImportError:\n",
    "        return False\n",
    "\n",
    "# Pacotes essenciais para embeddings\n",
    "pacotes_embeddings = {\n",
    "    'sentence_transformers': 'sentence_transformers',\n",
    "    'numpy': 'numpy',\n",
    "    'scikit_learn': 'sklearn'\n",
    "}\n",
    "\n",
    "print(\"ğŸ” Verificando dependÃªncias para embeddings...\")\n",
    "todos_ok = True\n",
    "for nome_display, nome_modulo in pacotes_embeddings.items():\n",
    "    status = verificar_instalacao(nome_modulo)\n",
    "    emoji = \"âœ…\" if status else \"âŒ\"\n",
    "    print(f\"{emoji} {nome_display}\")\n",
    "    if not status:\n",
    "        todos_ok = False\n",
    "\n",
    "if todos_ok:\n",
    "    print(\"\\nğŸ‰ Todas as dependÃªncias estÃ£o instaladas!\")\n",
    "else:\n",
    "    print(\"\\nğŸ’¡ Execute: pip install sentence-transformers scikit-learn\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ Status: {'âœ… PRONTO' if todos_ok else 'âš ï¸ VERIFICAR INSTALAÃ‡ÃƒO'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059f8c2b",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ **Carregamento da Base de Conhecimento**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7d3d29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Base carregada: 8822 bytes\n",
      "\n",
      "ğŸ“Š Total de caracteres: 8515\n",
      "ğŸ“ Primeiras 200 chars: # BASE DE CONHECIMENTO - FITNESS & MUSCULAÃ‡ÃƒO\n",
      "\n",
      "## EXERCÃCIOS POR GRUPO MUSCULAR\n",
      "\n",
      "### PEITO\n",
      "**Supino reto com barra**\n",
      "- MÃºsculos: Peitoral maior, deltÃ³ide anterior, trÃ­ceps\n",
      "- ExecuÃ§Ã£o: Deite no banco, ...\n"
     ]
    }
   ],
   "source": [
    "# ğŸ“„ CARREGAMENTO DA BASE FITNESS\n",
    "\n",
    "def carregar_base_conhecimento():\n",
    "    \"\"\"Carrega a base de conhecimento fitness do arquivo\"\"\"\n",
    "    arquivo_base = Path(\"base_conhecimento_fitness.txt\")\n",
    "    \n",
    "    if not arquivo_base.exists():\n",
    "        print(\"âš ï¸ Arquivo base_conhecimento_fitness.txt nÃ£o encontrado!\")\n",
    "        return criar_base_demo()\n",
    "    \n",
    "    try:\n",
    "        with open(arquivo_base, 'r', encoding='utf-8') as f:\n",
    "            conteudo = f.read()\n",
    "        print(f\"âœ… Base carregada: {arquivo_base.stat().st_size} bytes\")\n",
    "        return conteudo\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erro ao carregar: {e}\")\n",
    "        return criar_base_demo()\n",
    "\n",
    "def criar_base_demo():\n",
    "    \"\"\"Cria base mÃ­nima para demonstraÃ§Ã£o\"\"\"\n",
    "    return \"\"\"\n",
    "### EXERCÃCIOS BÃSICOS\n",
    "\n",
    "**Supino reto**\n",
    "ExercÃ­cio fundamental para peitoral maior, deltÃ³ide anterior e trÃ­ceps.\n",
    "\n",
    "**Agachamento**\n",
    "Rei dos exercÃ­cios para quadrÃ­ceps, glÃºteos e core.\n",
    "\n",
    "### HIPERTROFIA\n",
    "SÃ©ries: 3-4\n",
    "RepetiÃ§Ãµes: 8-12\n",
    "Descanso: 60-90 segundos\n",
    "\"\"\"\n",
    "\n",
    "# Carregar base\n",
    "conhecimento_bruto = carregar_base_conhecimento()\n",
    "print(f\"\\nğŸ“Š Total de caracteres: {len(conhecimento_bruto)}\")\n",
    "print(f\"ğŸ“ Primeiras 200 chars: {conhecimento_bruto[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c818ef",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ **Chunking Inteligente**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b810e771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ‚ï¸ Processando chunking...\n",
      "\n",
      "ğŸ“Š ESTATÃSTICAS DO CHUNKING:\n",
      "   â€¢ Total de chunks: 22\n",
      "   â€¢ Tokens mÃ©dios por chunk: 55.4\n",
      "   â€¢ SeÃ§Ãµes identificadas: 22\n",
      "\n",
      "ğŸ“„ EXEMPLOS DE CHUNKS:\n",
      "\n",
      "   Chunk 1 (ID: 0, Hash: 01b4d409)\n",
      "   SeÃ§Ã£o: IntroduÃ§Ã£o\n",
      "   Tokens: 13\n",
      "   Texto: # BASE DE CONHECIMENTO - FITNESS & MUSCULAÃ‡ÃƒO\n",
      "\n",
      "## EXERCÃCIOS POR GRUPO MUSCULAR...\n",
      "\n",
      "   Chunk 2 (ID: 1, Hash: 85a251ba)\n",
      "   SeÃ§Ã£o: PEITO\n",
      "   Tokens: 168\n",
      "   Texto: ### PEITO\n",
      "**Supino reto com barra**\n",
      "- MÃºsculos: Peitoral maior, deltÃ³ide anterior, trÃ­ceps\n",
      "- ExecuÃ§Ã£...\n",
      "\n",
      "   Chunk 3 (ID: 2, Hash: ca945e8b)\n",
      "   SeÃ§Ã£o: COSTAS\n",
      "   Tokens: 152\n",
      "   Texto: ### COSTAS\n",
      "**Puxada frontal**\n",
      "- MÃºsculos: Grande dorsal, rombÃ³ides, bÃ­ceps\n",
      "- ExecuÃ§Ã£o: Puxe a barra ...\n"
     ]
    }
   ],
   "source": [
    "# âœ‚ï¸ SISTEMA DE CHUNKING INTELIGENTE\n",
    "\n",
    "def fazer_chunking(texto: str, chunk_size: int = 300) -> List[Dict]:\n",
    "    \"\"\"Divide texto em chunks preservando contexto semÃ¢ntico\"\"\"\n",
    "    chunks = []\n",
    "    linhas = texto.split('\\n')\n",
    "    \n",
    "    chunk_atual = \"\"\n",
    "    secao_atual = \"IntroduÃ§Ã£o\"\n",
    "    \n",
    "    for linha in linhas:\n",
    "        linha = linha.strip()\n",
    "        \n",
    "        # Detectar nova seÃ§Ã£o (### TÃ­tulo)\n",
    "        if linha.startswith('### '):\n",
    "            # Salvar chunk anterior\n",
    "            if chunk_atual.strip():\n",
    "                chunks.append({\n",
    "                    'id': len(chunks),\n",
    "                    'texto': chunk_atual.strip(),\n",
    "                    'secao': secao_atual,\n",
    "                    'tokens': len(chunk_atual.split()),\n",
    "                    'hash': hashlib.md5(chunk_atual.encode()).hexdigest()[:8]\n",
    "                })\n",
    "            \n",
    "            # Nova seÃ§Ã£o\n",
    "            secao_atual = linha.replace('### ', '').strip()\n",
    "            chunk_atual = linha + \"\\n\"\n",
    "            \n",
    "        else:\n",
    "            chunk_atual += linha + \"\\n\"\n",
    "            \n",
    "            # Dividir se muito grande\n",
    "            if len(chunk_atual.split()) > chunk_size:\n",
    "                chunks.append({\n",
    "                    'id': len(chunks),\n",
    "                    'texto': chunk_atual.strip(),\n",
    "                    'secao': secao_atual,\n",
    "                    'tokens': len(chunk_atual.split()),\n",
    "                    'hash': hashlib.md5(chunk_atual.encode()).hexdigest()[:8]\n",
    "                })\n",
    "                chunk_atual = \"\"\n",
    "    \n",
    "    # Ãšltimo chunk\n",
    "    if chunk_atual.strip():\n",
    "        chunks.append({\n",
    "            'id': len(chunks),\n",
    "            'texto': chunk_atual.strip(),\n",
    "            'secao': secao_atual,\n",
    "            'tokens': len(chunk_atual.split()),\n",
    "            'hash': hashlib.md5(chunk_atual.encode()).hexdigest()[:8]\n",
    "        })\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Processar chunking\n",
    "print(\"âœ‚ï¸ Processando chunking...\")\n",
    "chunks = fazer_chunking(conhecimento_bruto)\n",
    "\n",
    "print(f\"\\nğŸ“Š ESTATÃSTICAS DO CHUNKING:\")\n",
    "print(f\"   â€¢ Total de chunks: {len(chunks)}\")\n",
    "print(f\"   â€¢ Tokens mÃ©dios por chunk: {sum(c['tokens'] for c in chunks) / len(chunks):.1f}\")\n",
    "print(f\"   â€¢ SeÃ§Ãµes identificadas: {len(set(c['secao'] for c in chunks))}\")\n",
    "\n",
    "# Mostrar alguns chunks\n",
    "print(f\"\\nğŸ“„ EXEMPLOS DE CHUNKS:\")\n",
    "for i, chunk in enumerate(chunks[:3]):\n",
    "    print(f\"\\n   Chunk {i+1} (ID: {chunk['id']}, Hash: {chunk['hash']})\")\n",
    "    print(f\"   SeÃ§Ã£o: {chunk['secao']}\")\n",
    "    print(f\"   Tokens: {chunk['tokens']}\")\n",
    "    print(f\"   Texto: {chunk['texto'][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3a0073",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ **GeraÃ§Ã£o de Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5d8adfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Carregando modelo de embeddings...\n",
      "âœ… Modelo carregado com sucesso!\n",
      "ğŸ“ DimensÃ£o dos embeddings: 384\n"
     ]
    }
   ],
   "source": [
    "# ğŸ§® SISTEMA DE EMBEDDINGS\n",
    "\n",
    "# Importar SentenceTransformers\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    \n",
    "    print(\"ğŸ”„ Carregando modelo de embeddings...\")\n",
    "    # Modelo multilingual otimizado\n",
    "    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    print(\"âœ… Modelo carregado com sucesso!\")\n",
    "    print(f\"ğŸ“ DimensÃ£o dos embeddings: {embedding_model.get_sentence_embedding_dimension()}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"âŒ SentenceTransformers nÃ£o disponÃ­vel\")\n",
    "    embedding_model = None\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Erro ao carregar modelo: {e}\")\n",
    "    embedding_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb4c2265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§® Gerando embeddings para todos os chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75c2f5bd4ea347aeba8537c976d3968f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Embeddings gerados!\n",
      "ğŸ“Š Shape dos embeddings: (22, 384)\n",
      "ğŸ“ DimensÃµes: 384\n",
      "ğŸ¯ Norma mÃ©dia: 1.000\n"
     ]
    }
   ],
   "source": [
    "# ğŸ¯ GERAÃ‡ÃƒO DE EMBEDDINGS VETORIAIS\n",
    "\n",
    "if embedding_model:\n",
    "    print(\"ğŸ§® Gerando embeddings para todos os chunks...\")\n",
    "    \n",
    "    # Extrair textos dos chunks\n",
    "    textos = [chunk['texto'] for chunk in chunks]\n",
    "    \n",
    "    # Gerar embeddings em batch (mais eficiente)\n",
    "    embeddings = embedding_model.encode(textos, show_progress_bar=True)\n",
    "    \n",
    "    # Adicionar embeddings aos chunks\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk['embedding'] = embeddings[i]\n",
    "        chunk['embedding_norm'] = np.linalg.norm(embeddings[i])  # Norma para referÃªncia\n",
    "    \n",
    "    print(f\"\\nâœ… Embeddings gerados!\")\n",
    "    print(f\"ğŸ“Š Shape dos embeddings: {embeddings.shape}\")\n",
    "    print(f\"ğŸ“ DimensÃµes: {embeddings.shape[1]}\")\n",
    "    print(f\"ğŸ¯ Norma mÃ©dia: {np.mean([c['embedding_norm'] for c in chunks]):.3f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ Pulando geraÃ§Ã£o de embeddings - modelo nÃ£o disponÃ­vel\")\n",
    "    embeddings = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62adfc3c",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ **Testes de Similaridade**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c769c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… FunÃ§Ãµes de busca criadas!\n"
     ]
    }
   ],
   "source": [
    "# ğŸ” FUNÃ‡ÃƒO DE BUSCA POR SIMILARIDADE\n",
    "\n",
    "def buscar_similares(query: str, chunks: List[Dict], modelo, top_k: int = 3) -> List[Dict]:\n",
    "    \"\"\"Busca chunks similares usando similaridade coseno\"\"\"\n",
    "    \n",
    "    if not modelo or not chunks or 'embedding' not in chunks[0]:\n",
    "        print(\"âŒ Embeddings nÃ£o disponÃ­veis - usando busca por palavras\")\n",
    "        return buscar_palavras_chave(query, chunks, top_k)\n",
    "    \n",
    "    try:\n",
    "        # Gerar embedding da query\n",
    "        query_embedding = modelo.encode([query])[0]\n",
    "        \n",
    "        # Calcular similaridades\n",
    "        similaridades = []\n",
    "        for chunk in chunks:\n",
    "            # Similaridade coseno\n",
    "            sim = np.dot(query_embedding, chunk['embedding']) / (\n",
    "                np.linalg.norm(query_embedding) * np.linalg.norm(chunk['embedding'])\n",
    "            )\n",
    "            similaridades.append((sim, chunk))\n",
    "        \n",
    "        # Ordenar por similaridade\n",
    "        similaridades.sort(key=lambda x: x[0], reverse=True)\n",
    "        \n",
    "        # Retornar top-k com scores\n",
    "        resultados = []\n",
    "        for sim, chunk in similaridades[:top_k]:\n",
    "            resultado = chunk.copy()\n",
    "            resultado['similaridade'] = float(sim)\n",
    "            resultados.append(resultado)\n",
    "        \n",
    "        return resultados\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erro na busca semÃ¢ntica: {e}\")\n",
    "        return buscar_palavras_chave(query, chunks, top_k)\n",
    "\n",
    "def buscar_palavras_chave(query: str, chunks: List[Dict], top_k: int = 3) -> List[Dict]:\n",
    "    \"\"\"Busca por palavras-chave como fallback\"\"\"\n",
    "    query_lower = query.lower()\n",
    "    query_words = set(query_lower.split())\n",
    "    \n",
    "    resultados = []\n",
    "    for chunk in chunks:\n",
    "        texto_lower = chunk['texto'].lower()\n",
    "        chunk_words = set(texto_lower.split())\n",
    "        \n",
    "        # Score baseado em palavras comuns\n",
    "        palavras_comuns = len(query_words.intersection(chunk_words))\n",
    "        if palavras_comuns > 0:\n",
    "            score = palavras_comuns / len(query_words)\n",
    "            resultado = chunk.copy()\n",
    "            resultado['similaridade'] = score\n",
    "            resultados.append(resultado)\n",
    "    \n",
    "    # Ordenar por score\n",
    "    resultados.sort(key=lambda x: x['similaridade'], reverse=True)\n",
    "    return resultados[:top_k]\n",
    "\n",
    "print(\"âœ… FunÃ§Ãµes de busca criadas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c7760f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª TESTANDO BUSCA SEMÃ‚NTICA\n",
      "==================================================\n",
      "\n",
      "1ï¸âƒ£ Query: 'exercÃ­cios para ganhar mÃºsculos'\n",
      "------------------------------\n",
      "\n",
      "   ğŸ“„ Resultado 1:\n",
      "   ğŸ“Š Similaridade: 0.575\n",
      "   ğŸ“‚ SeÃ§Ã£o: IntroduÃ§Ã£o\n",
      "   ğŸ“ Texto: # BASE DE CONHECIMENTO - FITNESS & MUSCULAÃ‡ÃƒO\n",
      "\n",
      "## EXERCÃCIOS POR GRUPO MUSCULAR...\n",
      "\n",
      "   ğŸ“„ Resultado 2:\n",
      "   ğŸ“Š Similaridade: 0.519\n",
      "   ğŸ“‚ SeÃ§Ã£o: PERNAS\n",
      "   ğŸ“ Texto: ### PERNAS\n",
      "\n",
      "#### QUADRÃCEPS\n",
      "**Agachamento**\n",
      "- MÃºsculos: QuadrÃ­ceps, glÃºteos, core\n",
      "- ExecuÃ§Ã£o: Descida controlada atÃ© 90Â°...\n",
      "\n",
      "2ï¸âƒ£ Query: 'treino de peito'\n",
      "------------------------------\n",
      "\n",
      "   ğŸ“„ Resultado 1:\n",
      "   ğŸ“Š Similaridade: 0.462\n",
      "   ğŸ“‚ SeÃ§Ã£o: FORÃ‡A\n",
      "   ğŸ“ Texto: ### FORÃ‡A\n",
      "- **Volume**: Moderado (3-5 sÃ©ries, 1-6 repetiÃ§Ãµes)\n",
      "- **Intensidade**: Alta (85-100% 1RM)\n",
      "- **FrequÃªncia**: 3-...\n",
      "\n",
      "   ğŸ“„ Resultado 2:\n",
      "   ğŸ“Š Similaridade: 0.459\n",
      "   ğŸ“‚ SeÃ§Ã£o: CONDICIONAMENTO\n",
      "   ğŸ“ Texto: ### CONDICIONAMENTO\n",
      "- **Volume**: Alto\n",
      "- **Intensidade**: Variada\n",
      "- **FrequÃªncia**: 5-6x por semana\n",
      "- **Modalidades**: H...\n",
      "\n",
      "3ï¸âƒ£ Query: 'quantas sÃ©ries fazer'\n",
      "------------------------------\n",
      "\n",
      "   ğŸ“„ Resultado 1:\n",
      "   ğŸ“Š Similaridade: 0.320\n",
      "   ğŸ“‚ SeÃ§Ã£o: FORÃ‡A\n",
      "   ğŸ“ Texto: ### FORÃ‡A\n",
      "- **Volume**: Moderado (3-5 sÃ©ries, 1-6 repetiÃ§Ãµes)\n",
      "- **Intensidade**: Alta (85-100% 1RM)\n",
      "- **FrequÃªncia**: 3-...\n",
      "\n",
      "   ğŸ“„ Resultado 2:\n",
      "   ğŸ“Š Similaridade: 0.252\n",
      "   ğŸ“‚ SeÃ§Ã£o: HIPERTROFIA\n",
      "   ğŸ“ Texto: ### HIPERTROFIA\n",
      "- **Volume**: Alto (3-4 sÃ©ries, 8-12 repetiÃ§Ãµes)\n",
      "- **Intensidade**: Moderada a alta (70-85% 1RM)\n",
      "- **Fre...\n",
      "\n",
      "4ï¸âƒ£ Query: 'descanso entre exercÃ­cios'\n",
      "------------------------------\n",
      "\n",
      "   ğŸ“„ Resultado 1:\n",
      "   ğŸ“Š Similaridade: 0.473\n",
      "   ğŸ“‚ SeÃ§Ã£o: IntroduÃ§Ã£o\n",
      "   ğŸ“ Texto: # BASE DE CONHECIMENTO - FITNESS & MUSCULAÃ‡ÃƒO\n",
      "\n",
      "## EXERCÃCIOS POR GRUPO MUSCULAR...\n",
      "\n",
      "   ğŸ“„ Resultado 2:\n",
      "   ğŸ“Š Similaridade: 0.455\n",
      "   ğŸ“‚ SeÃ§Ã£o: GASTO CALÃ“RICO APROXIMADO POR TREINO\n",
      "   ğŸ“ Texto: ### GASTO CALÃ“RICO APROXIMADO POR TREINO\n",
      "**MusculaÃ§Ã£o (por hora):**\n",
      "- Pessoa 70kg: ~220-400 calorias\n",
      "- Pessoa 80kg: ~252...\n",
      "\n",
      "==================================================\n",
      "âœ… Testes de similaridade concluÃ­dos!\n"
     ]
    }
   ],
   "source": [
    "# ğŸ§ª TESTES DE BUSCA SEMÃ‚NTICA\n",
    "\n",
    "print(\"ğŸ§ª TESTANDO BUSCA SEMÃ‚NTICA\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Queries de teste\n",
    "queries_teste = [\n",
    "    \"exercÃ­cios para ganhar mÃºsculos\",\n",
    "    \"treino de peito\", \n",
    "    \"quantas sÃ©ries fazer\",\n",
    "    \"descanso entre exercÃ­cios\"\n",
    "]\n",
    "\n",
    "for i, query in enumerate(queries_teste, 1):\n",
    "    print(f\"\\n{i}ï¸âƒ£ Query: '{query}'\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    resultados = buscar_similares(query, chunks, embedding_model, top_k=2)\n",
    "    \n",
    "    for j, resultado in enumerate(resultados, 1):\n",
    "        print(f\"\\n   ğŸ“„ Resultado {j}:\")\n",
    "        print(f\"   ğŸ“Š Similaridade: {resultado['similaridade']:.3f}\")\n",
    "        print(f\"   ğŸ“‚ SeÃ§Ã£o: {resultado['secao']}\")\n",
    "        print(f\"   ğŸ“ Texto: {resultado['texto'][:120]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"âœ… Testes de similaridade concluÃ­dos!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6425bcb1",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ **PersistÃªncia dos Dados**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24700cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chunks salvos em fitness_chunks.json\n",
      "ğŸ“Š Total: 22 chunks\n"
     ]
    }
   ],
   "source": [
    "# ğŸ’¾ SALVAR DADOS PROCESSADOS\n",
    "\n",
    "def salvar_chunks_processados(chunks: List[Dict], arquivo: str = \"fitness_chunks.json\"):\n",
    "    \"\"\"Salva chunks processados em JSON (sem embeddings)\"\"\"\n",
    "    \n",
    "    # Preparar dados para salvamento (sem embeddings numpy)\n",
    "    chunks_para_salvar = []\n",
    "    for chunk in chunks:\n",
    "        chunk_limpo = {\n",
    "            'id': chunk['id'],\n",
    "            'texto': chunk['texto'],\n",
    "            'secao': chunk['secao'], \n",
    "            'tokens': chunk['tokens'],\n",
    "            'hash': chunk['hash'],\n",
    "            'tem_embedding': 'embedding' in chunk\n",
    "        }\n",
    "        chunks_para_salvar.append(chunk_limpo)\n",
    "    \n",
    "    # Salvar metadata\n",
    "    dados = {\n",
    "        'timestamp': str(pd.Timestamp.now()) if 'pd' in globals() else 'unknown',\n",
    "        'total_chunks': len(chunks),\n",
    "        'modelo_embedding': 'all-MiniLM-L6-v2' if embedding_model else None,\n",
    "        'chunks': chunks_para_salvar\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        with open(arquivo, 'w', encoding='utf-8') as f:\n",
    "            json.dump(dados, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"âœ… Chunks salvos em {arquivo}\")\n",
    "        print(f\"ğŸ“Š Total: {len(chunks)} chunks\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erro ao salvar: {e}\")\n",
    "\n",
    "# Salvar se temos chunks\n",
    "if chunks:\n",
    "    salvar_chunks_processados(chunks)\n",
    "else:\n",
    "    print(\"âš ï¸ Nenhum chunk para salvar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a956c91d",
   "metadata": {},
   "source": [
    "## ğŸ“Š **Resumo da Etapa 01**\n",
    "\n",
    "### âœ… **Conquistas:**\n",
    "- ğŸ“„ Base de conhecimento carregada e processada\n",
    "- âœ‚ï¸ Chunking inteligente implementado\n",
    "- ğŸ§® Embeddings vetoriais gerados\n",
    "- ğŸ” Sistema de busca por similaridade funcional\n",
    "- ğŸ’¾ PersistÃªncia de dados configurada\n",
    "\n",
    "---\n",
    "\n",
    "ğŸ¯ **Base vetorial fitness criada com sucesso!** ğŸ‹ï¸â€â™‚ï¸"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
